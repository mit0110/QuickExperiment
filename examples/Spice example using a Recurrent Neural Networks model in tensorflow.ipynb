{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network model for regression in sequences\n",
    "====\n",
    "\n",
    "In this example we will use the RNNModel to set up an experiment over one of the Spice (http://spice.lif.univ-mrs.fr/index.php) competence for sequence prediction, held in 2016. We will start by downloading and preprocessing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ORIGIN_URL = 'http://spice.lif.univ-mrs.fr/data/2.spice.train'\n",
    "DATASET_DIR = 'downloads'\n",
    "DATASET_FILENAME = 'spice_dataset2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add parent directory to python path\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import urllib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.safe_mkdir(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maybe_download():\n",
    "    \"\"\"Downloads dataset if it doesn't exists\"\"\"\n",
    "    filename = os.path.join(DATASET_DIR, DATASET_FILENAME)\n",
    "    if os.path.exists(filename):\n",
    "        return\n",
    "    urllib.urlretrieve(ORIGIN_URL, filename)\n",
    "\n",
    "maybe_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset file consists on a series of numerical sequences, one per line, including a header line that we will ignore. We will try to predict the last element of each sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "def get_instances(elements, sequences):\n",
    "    \"\"\"Converts the elements to one hot encodings and divides them according to sequences.\"\"\"\n",
    "    encoder = OneHotEncoder()\n",
    "    elements = encoder.fit_transform(elements)\n",
    "    instances = []\n",
    "    for start, end in sequences:\n",
    "        instances.append(elements[start:end])\n",
    "    return instances\n",
    "\n",
    "\n",
    "def read_dataset():\n",
    "    \"\"\"Reads the dataset. Returns a list with sequences and a list of labels\"\"\"\n",
    "    with open(os.path.join(DATASET_DIR, DATASET_FILENAME), 'r') as input_file:\n",
    "        lines = input_file.readlines()[1:]  # Ignore the header\n",
    "    # Split lines and convert numbers to one hot encodings.\n",
    "    sequences = []  # A list with start, end of each sequence.\n",
    "    elements = []\n",
    "    labels = []\n",
    "    current_start = 0\n",
    "    for line in lines:\n",
    "        values = line.split()\n",
    "        # We discard the first element (sequence lenght) and the last one (sequence label)\n",
    "        sequences.append((current_start, current_start + len(values) - 2))\n",
    "        current_start += len(values) - 2\n",
    "        for value in values[1:-1]:\n",
    "            elements.append([int(value)])\n",
    "        labels.append(values[-1])\n",
    "    instances = get_instances(elements, sequences)\n",
    "    return numpy.array(instances), numpy.array(labels)\n",
    "\n",
    "instances, labels = read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the dataset using the extracted instances and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "dataset = reload(dataset)\n",
    "\n",
    "samples = 1\n",
    "partition_sizes = {'train': 0.7, 'test': 0.2, 'validation': 0.1}\n",
    "\n",
    "splice_dataset = dataset.SequenceDataset()\n",
    "splice_dataset.create_samples(instances, labels, samples, partition_sizes, use_numeric_labels=True)\n",
    "logs_dirname = '../../results/examples/splice/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove previous directory\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(logs_dirname)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import experiment\n",
    "experiment = reload(experiment)\n",
    "from models import lstm, mlp\n",
    "mlp = reload(mlp)\n",
    "lstm = reload(lstm)\n",
    "\n",
    "utils.safe_mkdir(logs_dirname)\n",
    "\n",
    "config = {\n",
    "    'model': lstm.LSTMModel,\n",
    "    'model_arguments': {'hidden_layer_size': 20, 'batch_size': 500,\n",
    "                        'logs_dirname': logs_dirname,\n",
    "                        'log_values': 100, 'training_epochs': 1000, 'max_num_steps': 10}\n",
    "}\n",
    "splice_experiment = experiment.SampledExperiment(splice_dataset, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "splice_experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are obtaining a very low accuracy, let's see what's going on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions, true = splice_experiment.model.predict('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true=true, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print numpy.unique(true, return_counts=True)\n",
    "print predictions, numpy.unique(predictions, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see too much difference between the validation and test accuracy, and the labels predicted are \"evenly\" distributed. This tell us that the problem is the network does not have enough information to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Seq2Seq prediction\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome the problem above, we are going to train now a model that predicts the next element on the sequence. We need to re-process the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def get_instances(elements, sequences):\n",
    "    \"\"\"Converts the elements to one hot encodings and divides them according to sequences.\"\"\"\n",
    "    encoder = OneHotEncoder()\n",
    "    elements = encoder.fit_transform(elements)\n",
    "    instances = []\n",
    "    for start, end in sequences:\n",
    "        instances.append(elements[start:end])\n",
    "    return instances\n",
    "\n",
    "\n",
    "def read_dataset():\n",
    "    \"\"\"Reads the dataset. Returns a list with sequences and a list of labels\"\"\"\n",
    "    with open(os.path.join(DATASET_DIR, DATASET_FILENAME), 'r') as input_file:\n",
    "        lines = input_file.readlines()[1:]  # Ignore the header\n",
    "    # Split lines and convert numbers to one hot encodings.\n",
    "    sequences = []  # A list with start, end of each sequence.\n",
    "    elements = []\n",
    "    labels = []\n",
    "    current_start = 0\n",
    "    for line in lines:\n",
    "        values = line.split()\n",
    "        # We discard the first element (sequence lenght)\n",
    "        sequences.append((current_start, current_start + len(values) - 1))\n",
    "        current_start += len(values) - 1\n",
    "        for value in values[1:]:\n",
    "            elements.append([int(value)])\n",
    "    instances = get_instances(elements, sequences)\n",
    "    return numpy.array(instances)\n",
    "\n",
    "instances = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ <44x10 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 44 stored elements in Compressed Sparse Row format>,\n",
       "       <23x10 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 23 stored elements in Compressed Sparse Row format>,\n",
       "       <13x10 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>,\n",
       "       ...,\n",
       "       <21x10 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>,\n",
       "       <25x10 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 25 stored elements in Compressed Sparse Row format>,\n",
       "       <37x10 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 37 stored elements in Compressed Sparse Row format>], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "dataset = reload(dataset)\n",
    "\n",
    "samples = 1\n",
    "partition_sizes = {'train': 0.7, 'test': 0.2, 'validation': 0.1}\n",
    "\n",
    "unlabeled_splice_dataset = dataset.UnlabeledSequenceDataset()\n",
    "unlabeled_splice_dataset.create_samples(instances, None, samples, partition_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import experiment\n",
    "experiment = reload(experiment)\n",
    "from models import lstm\n",
    "lstm = reload(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_dirname = '../../results/examples/splice/sequence'\n",
    "utils.safe_mkdir(logs_dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import logging\n",
    "\n",
    "class SequenceExperiment(experiment.SampledExperiment):\n",
    "\n",
    "    def _get_metrics(self, predictions):\n",
    "        \"\"\"Logs the values of the metric for the given predictions.\n",
    "        \n",
    "        Args:\n",
    "            predictions: 2-uple. Both elements are arrays of shape [batch_size, sequence_lengths].\n",
    "        \"\"\"\n",
    "        metric_values = []\n",
    "        for true, prediction in predictions:\n",
    "            metric_values.append(metrics.precision_recall_fscore_support(\n",
    "                numpy.concatenate(true), numpy.concatenate(prediction), average='micro'\n",
    "            )[:-1])\n",
    "        metric_values = numpy.array(metric_values)\n",
    "        report = ('\\n\\tPrecision\\tRecall\\tF1 Score\\n' + 'mean\\t' +\n",
    "            '\\t'.join([str(x) for x in metric_values.mean(axis=0)]) +\n",
    "            '\\nstd\\t' + '\\t'.join([str(x) for x in metric_values.std(axis=0)])\n",
    "        )\n",
    "        logging.info(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model': lstm.SeqPredictionModel,\n",
    "    'model_arguments': {'hidden_layer_size': 20, 'batch_size': 500,\n",
    "                        'logs_dirname': None,\n",
    "                        'log_values': 100, 'training_epochs': 1000, 'max_num_steps': 20}\n",
    "}\n",
    "sequence_splice_experiment = SequenceExperiment(unlabeled_splice_dataset, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Classifier loss at step 50000: 1.44864821434\n",
      "INFO:root:Validation accuracy 0.0855234563351\n",
      "INFO:root:Classifier loss at step 100000: 1.49653470516\n",
      "INFO:root:Validation accuracy 0.0924467816949\n",
      "INFO:root:Classifier loss at step 150000: 1.4352465868\n",
      "INFO:root:Validation accuracy 0.103144004941\n",
      "INFO:root:Classifier loss at step 200000: 1.53812026978\n",
      "INFO:root:Validation accuracy 0.117533668876\n",
      "INFO:root:Classifier loss at step 250000: 1.4862190485\n",
      "INFO:root:Validation accuracy 0.131190270185\n",
      "INFO:root:Classifier loss at step 300000: 1.46599245071\n",
      "INFO:root:Validation accuracy 0.139905512333\n",
      "INFO:root:Classifier loss at step 350000: 1.39895987511\n",
      "INFO:root:Validation accuracy 0.144276723266\n",
      "INFO:root:Classifier loss at step 400000: 1.47981023788\n",
      "INFO:root:Validation accuracy 0.148104906082\n",
      "INFO:root:Classifier loss at step 450000: 1.50725269318\n",
      "INFO:root:Validation accuracy 0.152394652367\n",
      "INFO:root:\n",
      "\tPrecision\tRecall\tF1 Score\n",
      "mean\t0.155376909113\t0.155376909113\t0.155376909113\n",
      "std\t0.0\t0.0\t0.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sequence_splice_experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions, true = sequence_splice_experiment.model.predict('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ array([ 2,  6,  1, 10,  3,  7,  1,  6,  9,  8,  3,  3,  9, 10,  8, 10,  4]),\n",
       "       array([ 6,  6,  1,  3,  6,  9,  2,  8,  9,  8,  8,  6, 10, 10,  7,  6,  7,\n",
       "        6,  5, 10]),\n",
       "       array([ 6,  6,  2,  4, 10,  9,  5,  2,  9,  9,  3,  6, 10,  6,  8,  6,  4,\n",
       "        5,  6, 10]),\n",
       "       array([ 8,  6,  3,  4,  3,  5,  8,  0,  5, 10,  8,  4,  4,  2,  8,  3,  1,\n",
       "        7,  4, 10]),\n",
       "       array([6, 6, 1, 4, 6, 8, 6, 2]),\n",
       "       array([ 6,  6,  3,  6,  3,  5,  1,  8,  9,  8,  3,  6,  9, 10,  7,  6,  4,\n",
       "        6,  6, 10]),\n",
       "       array([ 8,  4,  2,  4,  6,  9,  5,  8,  5,  3,  8, 10, 10,  0,  8,  0,  2,\n",
       "        5,  6, 10]),\n",
       "       array([6, 6, 3, 4, 3, 8, 8, 1, 9, 8]),\n",
       "       array([ 7,  6,  7,  3, 10,  9,  2,  8,  5,  3,  3,  6,  0,  5,  8, 10,  4,\n",
       "        6,  1, 10]),\n",
       "       array([ 8,  0,  3, 10,  3,  0,  6,  6,  5])], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ array([  8.,   6.,   8.,   0.,   2.,   6.,   6.,   6.,   6.,   6.,   6.,\n",
       "         8.,   3.,   7.,   6.,   7.,  10.]),\n",
       "       array([  5.,   7.,   4.,   6.,   5.,   9.,   9.,   6.,   9.,   5.,   3.,\n",
       "         5.,   2.,   0.,   3.,   2.,   0.,   6.,   5.,  10.]),\n",
       "       array([  1.,   5.,   5.,   8.,   5.,   9.,   6.,   8.,   8.,   3.,   1.,\n",
       "         6.,   7.,   2.,   3.,   9.,   4.,   6.,   5.,  10.]),\n",
       "       array([  3.,   3.,   5.,   6.,   3.,   1.,   4.,   7.,   1.,   8.,   5.,\n",
       "         7.,   6.,   2.,   8.,   8.,   2.,   6.,   6.,  10.]),\n",
       "       array([  8.,   7.,   4.,   6.,   3.,   1.,   5.,  10.]),\n",
       "       array([  3.,   8.,   2.,   8.,   0.,   6.,   6.,   6.,   8.,   0.,   6.,\n",
       "         6.,   3.,   5.,   2.,   6.,   8.,   0.,   8.,  10.]),\n",
       "       array([  8.,   5.,   2.,   8.,   5.,   0.,   0.,   9.,   1.,   9.,   1.,\n",
       "         5.,   7.,   7.,   5.,   1.,   6.,   4.,   2.,  10.]),\n",
       "       array([  0.,   3.,   4.,   6.,   1.,   9.,   2.,   6.,   5.,  10.]),\n",
       "       array([  7.,   4.,   9.,   8.,   0.,   6.,   6.,   3.,   5.,   6.,   6.,\n",
       "         0.,   5.,   8.,   0.,   6.,   3.,   1.,   0.,  10.]),\n",
       "       array([  6.,   3.,   8.,   2.,   4.,   6.,   3.,   5.,  10.])], dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ array([ 6,  4,  0,  8,  8,  8,  5,  8,  9,  4,  0,  3,  9,  6,  2,  5,  3,\n",
       "        9,  8,  4,  5,  0,  7,  4,  4,  8,  4,  2,  8,  7,  6,  3,  7,  0,\n",
       "        1,  3,  7,  6,  4,  8,  1,  4,  7, 10]),\n",
       "       array([ 6,  0,  1,  6,  4,  3,  9,  1,  2,  8,  8,  3,  6,  4,  4,  9,  5,\n",
       "        0,  6,  3,  2,  0, 10]),\n",
       "       array([ 1,  8,  4,  6,  7,  4,  1,  5,  7,  1,  9,  9, 10]),\n",
       "       array([ 9,  0,  0,  8,  1,  6,  4,  4,  0,  4,  6,  8,  2,  2,  2,  8,  4,\n",
       "        8,  9,  7,  0,  6,  3,  0,  5,  8,  1,  3,  3,  4,  6,  1,  5,  4,\n",
       "        4,  1,  5,  6,  6,  6,  8,  8,  3,  8,  5,  3,  5,  6,  6,  7,  0,\n",
       "        5,  2,  7,  5,  5,  3,  6,  9,  0,  6,  5,  2,  4,  5,  2,  6,  3,\n",
       "        6,  1,  3,  2,  6,  3,  4,  6,  7,  6,  8,  8,  6,  4,  7,  3,  4,\n",
       "        7,  8,  8,  7,  6,  0,  6,  3, 10]),\n",
       "       array([ 0,  9,  0,  6,  4,  3,  8,  2,  7,  4,  8,  8,  6,  0,  5,  8, 10]),\n",
       "       array([ 6,  6,  9,  8,  6,  8,  9,  0,  1,  6,  6,  8,  5, 10]),\n",
       "       array([ 8,  3,  9,  6,  6,  6,  3,  8,  6,  6,  0,  1,  5,  0,  6,  1,  3,\n",
       "        6,  0,  4,  4,  2,  0,  4,  4,  5,  1,  2,  6,  0,  5,  8,  7,  4,\n",
       "        1,  8,  4,  0,  0,  9,  3,  0,  9,  3,  2,  5,  4,  1,  5,  0,  6,\n",
       "        6,  8,  0,  9,  0,  8,  5,  5,  3,  0,  4,  0,  8,  4,  6,  4,  8,\n",
       "        6,  5,  6,  5,  5,  1,  7, 10]),\n",
       "       array([ 2,  1,  3,  8,  2,  2,  9,  5,  0,  4,  3,  9,  3,  6,  6,  3,  5,\n",
       "        9,  0,  2,  1, 10]),\n",
       "       array([ 0,  6,  1,  6,  8,  8,  7,  6,  6,  9,  6,  4,  5,  5,  3,  0,  2,\n",
       "        1,  8,  1,  6,  6,  5,  8,  6,  7,  4,  3,  8,  4,  7,  5,  1,  2,\n",
       "        6,  2,  3,  8,  6,  8,  0,  8,  1,  6,  1,  9,  0,  5,  4,  9,  1,\n",
       "        3,  2,  4,  2,  1,  9,  2,  6, 10]),\n",
       "       array([ 8,  6,  6,  2,  5,  3,  6,  6,  0,  8,  5,  5,  8,  0,  3,  4, 10])], dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_splice_dataset._labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
