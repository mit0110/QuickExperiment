{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris Simple Example\n",
    "=====\n",
    "\n",
    "This example shows how to use QuickExperiment to define an experiment suite for quick iterations and high result reproducibility. It uses the sklearn Iris dataset and a simple Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset, which is already preprocess into a numeric matrix and an array of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first example, we are only going to train a classifier, evaluate it over a test portion and save the results. For this, we will need to use an instance of BaseDataset and define the Experiment configuration.\n",
    "\n",
    "Our dataset consitst in a 2-D numpy array representing the instances, and a vector representing the labels. We can use the class SimpleDataset to model our data.\n",
    "\n",
    "BaseDatasets are created to optimize the stored information for a dataset that will be used many times, and likely partitioned in many ways. Over the course of an investigation, numerous experiments will be run and re-runned on a dataset, each time creating training and evaluation partitions. Instead of saving an entire copy of the matrixes for every partition, BaseDatasets stores the matrix only once and keeps the indices of the instances assigned to each partition. This also allows to compare results between experiments better and keep track of where the instances of the original dataset are being used.\n",
    "\n",
    "Let's create our Dataset instance: first, we need to create the train/test split of the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "train_index, test_index = next(splitter.split(iris.data, iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this indices to create an instance of BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add parent directory to python path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from dataset import SimpleDataset\n",
    "\n",
    "iris_dataset = SimpleDataset()\n",
    "indices = {'train': train_index, 'test': test_index}\n",
    "iris_dataset.create_from_matrixes(iris.data, indices, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define now the experiment we want to run using a configuration dictionary. The class model.SKlearnModel provides a simple API to create models wrapping Scikit-learn learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import model\n",
    "model = reload(model)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "config = {\n",
    "    'model': model.SkleanrModel,\n",
    "    'model_arguments': {'model_class': LogisticRegression, 'sklearn_model_arguments': {'C': 0.5, 'n_jobs': 2}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import experiment\n",
    "experiment = reload(experiment)\n",
    "lr_experiment = experiment.Experiment(iris_dataset, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the experiments. The simple experiment just trains the classifier with the 'train' partition and prints the classification report for the 'test' partition predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\tPrecision\tRecall\tF1 Score\tSupport\n",
      "Class 0\t1.0\t1.0\t1.0\t10.0\n",
      "Class 1\t1.0\t0.8\t0.888888888889\t10.0\n",
      "Class 2\t0.833333333333\t1.0\t0.909090909091\t10.0\n"
     ]
    }
   ],
   "source": [
    "lr_experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Iris Sampled Example\n",
    "===\n",
    "\n",
    "The framework also allows to run experiments in multiple samples with the same command, and obtain global metrics. For this, we will use the SimpleSampledDataset and the SampledExperiment class. This classes will create the samples for us, and train/evaluate the classifier in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partition_sizes = {'train': 0.8, 'test': 0.2}\n",
    "samples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import SimpleSampledDataset\n",
    "\n",
    "iris_sampled_dataset = SimpleSampledDataset()\n",
    "iris_sampled_dataset.create_samples(iris.data, iris.target, samples, partition_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment = reload(experiment)\n",
    "\n",
    "# We use the same config as before\n",
    "config = {\n",
    "    'model': model.SkleanrModel,\n",
    "    'model_arguments': {'model_class': LogisticRegression, 'sklearn_model_arguments': {'C': 0.5, 'n_jobs': 2}}\n",
    "}\n",
    "iris_sampled_experiment = experiment.SampledExperiment(iris_sampled_dataset, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\tPrecision\tRecall\tF1 Score\n",
      "mean\t0.98\t0.98\t0.98\n",
      "std\t0.0163299316186\t0.0163299316186\t0.0163299316186\n"
     ]
    }
   ],
   "source": [
    "iris_sampled_experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
