{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network model for classification in sequences\n",
    "====\n",
    "\n",
    "In this example we will use the RNNModel to set up an experiment over the MINST dataset included in TensorFlow. Based in https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb\n",
    "\n",
    "To classify images using a reccurent neural network, we consider every image row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 steps for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add parent directory to python path\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import urllib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting downloads/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting downloads/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting downloads/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting downloads/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"downloads/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "dataset = reload(dataset)\n",
    "\n",
    "samples = 1\n",
    "\n",
    "class MNSTDataset(dataset.SimpleDataset):\n",
    "    def create_from_matrixes(self, mnist, unused=None, unused2=None):\n",
    "        \"\"\"Creates the dataset from a matrix and the set of indices.\n",
    "\n",
    "        Args:\n",
    "            matrix (:obj: iterable): the Mnist dataset.\n",
    "        \"\"\"\n",
    "        self.datasets['train'] = mnist.train\n",
    "        self.datasets['test'] = mnist.test\n",
    "        self.datasets['validation'] = mnist.validation\n",
    "        \n",
    "        # All sequences have the same lenght\n",
    "        self.sequences_lenghts = 28\n",
    "    \n",
    "    def next_batch(self, batch_size, partition_name='train', pad_sequences=False, max_sequence_length=None):\n",
    "        \"\"\"Generates batches of instances and labels from a partition.\n",
    "\n",
    "        If the size of the partition is exceeded, the partition and the labels\n",
    "        are shuffled to generate further batches.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): the size of each batch to generate.\n",
    "            partition_name (str): the name of the partition to create the\n",
    "                batches from.\n",
    "        \"\"\"\n",
    "        instances, labels = self.datasets[partition_name].next_batch(batch_size)\n",
    "        instances = instances.reshape((batch_size, self.feature_vector_size, self.sequences_lenghts))\n",
    "        lenghts = numpy.zeros((batch_size,)) + self.sequences_lenghts\n",
    "        return instances, labels, lenghts\n",
    "    \n",
    "    def classes_num(self, unused=None):\n",
    "        \"\"\"Returns the number of unique classes in a partition of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: the number of uniqu classes.\n",
    "        \"\"\"\n",
    "        return 10\n",
    "    \n",
    "    def traverse_dataset(self, batch_size, partition_name):\n",
    "        \"\"\"Generates batches of instances and labels from a partition.\n",
    "\n",
    "        The iterator ends when the dataset has been entirely returned.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): the size of each batch to generate.\n",
    "            partition_name (str): the name of the partition to create the\n",
    "                batches from.\n",
    "\n",
    "        Yields:\n",
    "            A tuple (instances, lenghts, labels) of batch_size, except in the last\n",
    "            iteration where the size can be smaller.\n",
    "        \"\"\"\n",
    "        total_elements = 0\n",
    "        while True:\n",
    "            this_batch = batch_size\n",
    "            if total_elements + batch_size >= self.datasets[partition_name].num_examples:\n",
    "                this_batch = self.datasets[partition_name].num_examples - total_elements\n",
    "            instances, labels = self.datasets[partition_name].next_batch(this_batch)\n",
    "            instances = instances.reshape((this_batch, self.feature_vector_size, self.sequences_lenghts))\n",
    "            lenghts = numpy.zeros((this_batch,)) + self.sequences_lenghts\n",
    "            total_elements += this_batch\n",
    "            yield instances, labels, lenghts\n",
    "            if total_elements >= self.datasets[partition_name].num_examples:\n",
    "                break\n",
    "    \n",
    "    def get_labels(self, partition_name='train'):\n",
    "        \"\"\"Returns the labels of the partition\n",
    "        Args:\n",
    "            partition_name (str): the name of the partition to get the\n",
    "                labels from.\n",
    "\n",
    "        Returns:\n",
    "            An iterable with labels.\n",
    "        \"\"\"\n",
    "        return numpy.argmax(self.datasets[partition_name].labels, axis=1)\n",
    "    \n",
    "    @property\n",
    "    def labels_type(self):\n",
    "        return tf.float64\n",
    "    \n",
    "    @property\n",
    "    def feature_vector_size(self):\n",
    "        return 28\n",
    "    \n",
    "    def num_examples(self, partition_name='train'):\n",
    "        return self.datasets[partition_name].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_dataset = MNSTDataset()\n",
    "mnist_dataset.create_from_matrixes(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from models import lstm, mlp\n",
    "mlp = reload(mlp)\n",
    "lstm = reload(lstm)\n",
    "\n",
    "class MnistLSTMModel(lstm.LSTMModel):\n",
    "    \"\"\"LSTM Model that handles labels as one hot encodings.\"\"\"\n",
    "    \n",
    "    def _build_loss(self, logits):\n",
    "        \"\"\"Calculates the loss from the logits and the labels.\n",
    "\n",
    "        Args:\n",
    "            logits: Logits tensor, float - [self.batch_size, NUM_CLASSES].\n",
    "            labels_placeholder: Labels tensor, float - [batch_size, NUM_CLASSES]\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=self.labels_placeholder, name='cross_entropy')\n",
    "        return tf.reduce_mean(cross_entropy, name='cross_entropy_mean_loss')\n",
    "    \n",
    "    def _build_inputs(self):\n",
    "        \"\"\"Generate placeholder variables to represent the input tensors.\"\"\"\n",
    "        # Placeholder for the inputs in a given iteration.\n",
    "        self.instances_placeholder = tf.placeholder(\n",
    "            tf.float32,\n",
    "            (None, None, self.dataset.feature_vector_size),\n",
    "            name='sequences_placeholder')\n",
    "        \n",
    "        self.lengths_placeholder = tf.placeholder(\n",
    "            tf.int32, (None, ), name='lengths_placeholder')\n",
    "\n",
    "        # Labels are now a one-hot encoding\n",
    "        self.labels_placeholder = tf.placeholder(\n",
    "            self.dataset.labels_type, (None, self.dataset.classes_num()),\n",
    "            name='labels_placeholder')\n",
    "    \n",
    "    def _build_evaluation(self, logits):\n",
    "        \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "        Args:\n",
    "            logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "            labels: Labels tensor, int32 - [batch_size, NUM_CLASSES].\n",
    "\n",
    "        Returns:\n",
    "            A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "            that were predicted correctly.\n",
    "        \"\"\"\n",
    "        correct = tf.equal(tf.argmax(logits,1), tf.argmax(self.labels_placeholder,1))\n",
    "        return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    \n",
    "    def _build_train_operation(self, loss):\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        return optimizer.minimize(loss)\n",
    "    \n",
    "    def predict(self, partition_name):                                           \n",
    "        predictions = []                                                         \n",
    "        true = []                                                                \n",
    "        with self.graph.as_default():                                            \n",
    "            for instances, labels, lengths in self.dataset.traverse_dataset(     \n",
    "                   self.batch_size, partition_name):                            \n",
    "                feed_dict = {                                                    \n",
    "                    self.instances_placeholder: instances,                       \n",
    "                    self.labels_placeholder: labels,                             \n",
    "                    self.lengths_placeholder: lengths                            \n",
    "                }                                                                \n",
    "                predictions.extend(self.sess.run(self.predictions,               \n",
    "                                                 feed_dict=feed_dict))           \n",
    "                true.append(numpy.argmax(labels, axis=1))                        \n",
    "        return numpy.array(predictions), numpy.concatenate(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the dataset using the extracted instances and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove previous directory\n",
    "import shutil\n",
    "logs_dirname = '../../results/examples/mnist/'\n",
    "try:\n",
    "    shutil.rmtree(logs_dirname)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "utils.safe_mkdir(logs_dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import experiment\n",
    "experiment = reload(experiment)\n",
    "\n",
    "config = {\n",
    "    'model': MnistLSTMModel,\n",
    "    'model_arguments': {'hidden_layer_size': 128, 'batch_size': 128,\n",
    "                        'logs_dirname': None,\n",
    "                        'log_values': 500, 'training_epochs': 500, 'max_num_steps': 28,\n",
    "                        'learning_rate': 0.001}\n",
    "}\n",
    "mnist_experiment = experiment.Experiment(mnist_dataset, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\tPrecision\tRecall\tF1 Score\tSupport\n",
      "Class 0\t0.988775510204\t0.903075489282\t0.943984413054\t1073.0\n",
      "Class 1\t0.984140969163\t0.980684811238\t0.982409850484\t1139.0\n",
      "Class 2\t0.956395348837\t0.935545023697\t0.945855294681\t1055.0\n",
      "Class 3\t0.946534653465\t0.876260311641\t0.910042836744\t1091.0\n",
      "Class 4\t0.923625254582\t0.936983471074\t0.930256410256\t968.0\n",
      "Class 5\t0.924887892377\t0.94070695553\t0.932730356133\t877.0\n",
      "Class 6\t0.942588726514\t0.966809421842\t0.954545454545\t934.0\n",
      "Class 7\t0.939688715953\t0.947058823529\t0.943359375\t1020.0\n",
      "Class 8\t0.797741273101\t0.941818181818\t0.863813229572\t825.0\n",
      "Class 9\t0.919722497522\t0.911591355599\t0.915638875185\t1018.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mnist_experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions, true = mnist_experiment.model.predict('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 8 1 ..., 2 3 4] (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([1073, 1139, 1055, 1091,  968,  877,  934, 1020,  825, 1018]))\n"
     ]
    }
   ],
   "source": [
    "print predictions, numpy.unique(predictions, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9335"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
