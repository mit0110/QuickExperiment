{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network model for classification in sequences\n",
    "====\n",
    "\n",
    "In this example we will use the RNNModel to set up an experiment over the MINST dataset included in TensorFlow. Based in https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb\n",
    "\n",
    "To classify images using a reccurent neural network, we consider every image row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 steps for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add parent directory to python path\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import urllib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from quick_experiment import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting downloads/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting downloads/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting downloads/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting downloads/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"downloads/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from quick_experiment  import dataset\n",
    "dataset = reload(dataset)\n",
    "\n",
    "samples = 1\n",
    "\n",
    "class MNSTDataset(dataset.SimpleDataset):\n",
    "    def create_from_matrixes(self, mnist, unused=None, unused2=None):\n",
    "        \"\"\"Creates the dataset from a matrix and the set of indices.\n",
    "\n",
    "        Args:\n",
    "            matrix (:obj: iterable): the Mnist dataset.\n",
    "        \"\"\"\n",
    "        self.datasets['train'] = mnist.train\n",
    "        self.datasets['test'] = mnist.test\n",
    "        self.datasets['validation'] = mnist.validation\n",
    "        \n",
    "        # All sequences have the same lenght\n",
    "        self.sequences_lenghts = 28\n",
    "    \n",
    "    def next_batch(self, batch_size, partition_name='train', pad_sequences=False,\n",
    "                   max_sequence_length=None, reshuffle=True):\n",
    "        \"\"\"Generates batches of instances and labels from a partition.\n",
    "\n",
    "        If the size of the partition is exceeded, the partition and the labels\n",
    "        are shuffled to generate further batches.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): the size of each batch to generate.\n",
    "            partition_name (str): the name of the partition to create the\n",
    "                batches from.\n",
    "        \"\"\"\n",
    "        instances, labels = self.datasets[partition_name].next_batch(\n",
    "            batch_size)\n",
    "        instances = instances.reshape((batch_size, self.feature_vector_size, self.sequences_lenghts))\n",
    "        lenghts = numpy.zeros((batch_size,)) + self.sequences_lenghts\n",
    "        return instances, labels, lenghts\n",
    "    \n",
    "    def classes_num(self, unused=None):\n",
    "        \"\"\"Returns the number of unique classes in a partition of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: the number of uniqu classes.\n",
    "        \"\"\"\n",
    "        return 10\n",
    "    \n",
    "    def traverse_dataset(self, batch_size, partition_name):\n",
    "        \"\"\"Generates batches of instances and labels from a partition.\n",
    "\n",
    "        The iterator ends when the dataset has been entirely returned.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): the size of each batch to generate.\n",
    "            partition_name (str): the name of the partition to create the\n",
    "                batches from.\n",
    "\n",
    "        Yields:\n",
    "            A tuple (instances, lenghts, labels) of batch_size, except in the last\n",
    "            iteration where the size can be smaller.\n",
    "        \"\"\"\n",
    "        total_elements = 0\n",
    "        while True:\n",
    "            this_batch = batch_size\n",
    "            if total_elements + batch_size >= self.datasets[partition_name].num_examples:\n",
    "                this_batch = self.datasets[partition_name].num_examples - total_elements\n",
    "            instances, labels = self.datasets[partition_name].next_batch(this_batch)\n",
    "            instances = instances.reshape((this_batch, self.feature_vector_size, self.sequences_lenghts))\n",
    "            lenghts = numpy.zeros((this_batch,)) + self.sequences_lenghts\n",
    "            total_elements += this_batch\n",
    "            yield instances, labels, lenghts\n",
    "            if total_elements >= self.datasets[partition_name].num_examples:\n",
    "                break\n",
    "    \n",
    "    def get_labels(self, partition_name='train'):\n",
    "        \"\"\"Returns the labels of the partition\n",
    "        Args:\n",
    "            partition_name (str): the name of the partition to get the\n",
    "                labels from.\n",
    "\n",
    "        Returns:\n",
    "            An iterable with labels.\n",
    "        \"\"\"\n",
    "        return numpy.argmax(self.datasets[partition_name].labels, axis=1)\n",
    "    \n",
    "    @property\n",
    "    def labels_type(self):\n",
    "        return tf.float64\n",
    "    \n",
    "    @property\n",
    "    def feature_vector_size(self):\n",
    "        return 28\n",
    "    \n",
    "    def num_examples(self, partition_name='train'):\n",
    "        return self.datasets[partition_name].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_dataset = MNSTDataset()\n",
    "mnist_dataset.create_from_matrixes(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from quick_experiment.models import lstm, mlp\n",
    "mlp = reload(mlp)\n",
    "lstm = reload(lstm)\n",
    "\n",
    "class MnistLSTMModel(lstm.LSTMModel):\n",
    "    \"\"\"LSTM Model that handles labels as one hot encodings.\"\"\"\n",
    "    \n",
    "    def _build_loss(self, logits):\n",
    "        \"\"\"Calculates the loss from the logits and the labels.\n",
    "\n",
    "        Args:\n",
    "            logits: Logits tensor, float - [self.batch_size, NUM_CLASSES].\n",
    "            labels_placeholder: Labels tensor, float - [batch_size, NUM_CLASSES]\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=self.labels_placeholder, name='cross_entropy')\n",
    "        return tf.reduce_mean(cross_entropy, name='cross_entropy_mean_loss')\n",
    "    \n",
    "    def _build_inputs(self):\n",
    "        \"\"\"Generate placeholder variables to represent the input tensors.\"\"\"\n",
    "        # Placeholder for the inputs in a given iteration.\n",
    "        self.instances_placeholder = tf.placeholder(\n",
    "            tf.float32,\n",
    "            (None, None, self.dataset.feature_vector_size),\n",
    "            name='sequences_placeholder')\n",
    "        \n",
    "        self.lengths_placeholder = tf.placeholder(\n",
    "            tf.int32, (None, ), name='lengths_placeholder')\n",
    "\n",
    "        # Labels are now a one-hot encoding\n",
    "        self.labels_placeholder = tf.placeholder(\n",
    "            self.dataset.labels_type, (None, self.dataset.classes_num()),\n",
    "            name='labels_placeholder')\n",
    "    \n",
    "    def _build_evaluation(self, logits):\n",
    "        \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "        Args:\n",
    "            logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "            labels: Labels tensor, int32 - [batch_size, NUM_CLASSES].\n",
    "\n",
    "        Returns:\n",
    "            A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "            that were predicted correctly.\n",
    "        \"\"\"\n",
    "        correct = tf.equal(tf.argmax(logits,1), tf.argmax(self.labels_placeholder,1))\n",
    "        return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    \n",
    "    def _build_train_operation(self, loss):\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        return optimizer.minimize(loss)\n",
    "    \n",
    "    def predict(self, partition_name):                                           \n",
    "        predictions = []                                                         \n",
    "        true = []                                                                \n",
    "        with self.graph.as_default():                                            \n",
    "            for instances, labels, lengths in self.dataset.traverse_dataset(     \n",
    "                   self.batch_size, partition_name):                            \n",
    "                feed_dict = {                                                    \n",
    "                    self.instances_placeholder: instances,                       \n",
    "                    self.labels_placeholder: labels,                             \n",
    "                    self.lengths_placeholder: lengths                            \n",
    "                }                                                                \n",
    "                predictions.extend(self.sess.run(self.predictions,               \n",
    "                                                 feed_dict=feed_dict))           \n",
    "                true.append(numpy.argmax(labels, axis=1))                        \n",
    "        return numpy.array(predictions), numpy.concatenate(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the dataset using the extracted instances and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove previous directory\n",
    "import shutil\n",
    "logs_dirname = '../../results/examples/mnist/'\n",
    "try:\n",
    "    shutil.rmtree(logs_dirname)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "utils.safe_mkdir(logs_dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from quick_experiment import experiment\n",
    "experiment = reload(experiment)\n",
    "\n",
    "config = {\n",
    "    'model': MnistLSTMModel,\n",
    "    'model_arguments': {'hidden_layer_size': 128, 'batch_size': 128,\n",
    "                        'logs_dirname': logs_dirname,\n",
    "                        'log_values': 50, 'training_epochs': 100, 'max_num_steps': 28,\n",
    "                        'learning_rate': 0.001}\n",
    "}\n",
    "mnist_experiment = experiment.Experiment(mnist_dataset, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting\n",
      "Classifier loss at step 50 (6400): 1.0837161541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8cb25bdcc663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmnist_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/milagro/edm/QuickExperiment/quick_experiment/experiment.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, save_model, save_predictions)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Train classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/milagro/edm/QuickExperiment/quick_experiment/models/mlp.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, partition_name, close_session)\u001b[0m\n\u001b[1;32m    207\u001b[0m                         \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                     )\n\u001b[0;32m--> 209\u001b[0;31m                     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                     \u001b[0;31m# if self.logs_dirname is not None:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0;31m#     tf.summary.scalar('accuracy', accuracy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/milagro/edm/QuickExperiment/quick_experiment/models/lstm.py\u001b[0m in \u001b[0;36mevaluate_validation\u001b[0;34m(self, correct_predictions)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             true_count += self.sess.run(correct_predictions,\n\u001b[0;32m--> 182\u001b[0;31m                                         feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_count\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/milagro/anaconda2/envs/edm_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/milagro/anaconda2/envs/edm_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/milagro/anaconda2/envs/edm_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/milagro/anaconda2/envs/edm_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/milagro/anaconda2/envs/edm_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mnist_experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions, true = mnist_experiment.model.predict('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 8 ..., 4 4 8] (array([0, 1, 2, 3, 4, 5, 6, 7, 8]), array([ 293,   26,  273,   97, 7027,   19,  230,   23, 2012]))\n"
     ]
    }
   ],
   "source": [
    "print predictions, numpy.unique(predictions, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.083799999999999999"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
